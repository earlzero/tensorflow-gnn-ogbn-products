{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3098a1e9-3ca3-4d48-b42f-b8ab2f7702e1",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Figure out how to display label for examples for validation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c205d800-aa97-4ec8-adbc-1a0b1aeffe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "from ogb.nodeproppred import NodePropPredDataset\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "from tensorflow_gnn import runner\n",
    "from typing import Mapping\n",
    "from tensorflow_gnn.experimental import sampler\n",
    "from tensorflow_gnn.models import mt_albis\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import functools\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed9e675-558f-44c7-be94-9ef6bcc16294",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NodePropPredDataset(name = \"ogbn-products\", root = 'dataset/')\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "NUM_TRAINING_SAMPLES=train_idx.shape[0]\n",
    "NUM_VALIDATION_SAMPLES = valid_idx.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff08c192-777b-4dda-8e70-6d016e575056",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, label = dataset[0]\n",
    "mask0=np.isin(graph[\"edge_index\"][0],train_idx)\n",
    "mask1=np.isin(graph[\"edge_index\"][1],train_idx)\n",
    "\n",
    "mask = mask0 & mask1\n",
    "indices = np.where(mask)[0]\n",
    "train_edge_index=graph[\"edge_index\"][:, indices]\n",
    "train_node_feat = graph[\"node_feat\"][train_idx,:]\n",
    "train_label = label[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c4a9d78-523c-4870-ab61-5db17576bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying validation set here\n",
    "graph, label = dataset[0]\n",
    "mask0=np.isin(graph[\"edge_index\"][0],valid_idx)\n",
    "mask1=np.isin(graph[\"edge_index\"][1],valid_idx)\n",
    "\n",
    "mask = mask0 & mask1\n",
    "indices = np.where(mask)[0]\n",
    "valid_edge_index=graph[\"edge_index\"][:, indices]\n",
    "valid_node_feat = graph[\"node_feat\"][valid_idx,:]\n",
    "valid_label = label[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bc4e94-0020-4eaf-9e7c-b450d02f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_schema = tfgnn.read_schema(\"graph_schema.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b192796-4c46-42d6-802b-637f7512aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_schema = tfgnn.read_schema(\"graph_schema.pbtxt\")\n",
    "# graph_spec = tfgnn.create_graph_spec_from_schema_pb(graph_schema)\n",
    "# train_dataset_provider = runner.TFRecordDatasetProvider(file_pattern=\"train.tfrecord\")\n",
    "# train_dataset = train_dataset_provider.get_dataset(context=tf.distribute.InputContext())\n",
    "# train_dataset = train_dataset.map(lambda serialized: tfgnn.parse_single_example(serialized=serialized, spec=graph_spec))\n",
    "# graph_tensor = train_dataset.get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9053666-1f72-4165-aef9-c99e35da4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_schema = tfgnn.read_schema(\"graph_schema.pbtxt\")\n",
    "\n",
    "graph, label = dataset[0]\n",
    "SIZE=2449029\n",
    "\n",
    "graph_tensor = tfgnn.GraphTensor.from_pieces(\n",
    "    node_sets={\n",
    "       \"product\": tfgnn.NodeSet.from_fields(\n",
    "           sizes = tf.constant([2449029]),\n",
    "           features={\n",
    "               \"id\": tf.range(0,SIZE),\n",
    "               \"feature\": tf.constant(graph[\"node_feat\"]),\n",
    "               \"label\": tf.constant(label)\n",
    "           }\n",
    "       )\n",
    "   },\n",
    "    edge_sets = {\n",
    "      \"bought_together\": tfgnn.EdgeSet.from_fields(\n",
    "          sizes = tf.constant([graph[\"edge_index\"].shape[1]]),\n",
    "          adjacency = tfgnn.Adjacency.from_indices(\n",
    "              source = (\"product\", tf.constant(graph[\"edge_index\"][0,:])),\n",
    "              target = (\"product\", tf.constant(graph[\"edge_index\"][1,:])),\n",
    "          )\n",
    "      )  \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a1a0aa8-5053-45c0-a047-5c587239d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_tensor = tfgnn.GraphTensor.from_pieces(\n",
    "    node_sets={\n",
    "       \"product\": tfgnn.NodeSet.from_fields(\n",
    "           sizes = tf.constant([train_idx.shape[0]]),\n",
    "           features={\n",
    "               \"id\": tf.constant(train_idx),\n",
    "               \"feature\": tf.constant(train_node_feat),\n",
    "               \"label\": tf.constant(train_label)\n",
    "           }\n",
    "       )\n",
    "   },\n",
    "    edge_sets = {\n",
    "      \"bought_together\": tfgnn.EdgeSet.from_fields(\n",
    "          sizes = tf.constant([train_edge_index.shape[1]]),\n",
    "          adjacency = tfgnn.Adjacency.from_indices(\n",
    "              source = (\"product\", tf.constant(train_edge_index[0,:])),\n",
    "              target = (\"product\", tf.constant(train_edge_index[1,:])),\n",
    "          )\n",
    "      )  \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e9bed97-cc2e-4b54-acdc-0299a6e2ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_graph_tensor = tfgnn.GraphTensor.from_pieces(\n",
    "    node_sets={\n",
    "       \"product\": tfgnn.NodeSet.from_fields(\n",
    "           sizes = tf.constant([valid_idx.shape[0]]),\n",
    "           features={\n",
    "               \"id\": tf.range(196615, 39323+196615),\n",
    "               \"feature\": tf.constant(valid_node_feat),\n",
    "               \"label\": tf.constant(valid_label)\n",
    "           }\n",
    "       )\n",
    "   },\n",
    "    edge_sets = {\n",
    "      \"bought_together\": tfgnn.EdgeSet.from_fields(\n",
    "          sizes = tf.constant([valid_edge_index.shape[1]]),\n",
    "          adjacency = tfgnn.Adjacency.from_indices(\n",
    "              source = (\"product\", tf.constant(valid_edge_index[0,:])),\n",
    "              target = (\"product\", tf.constant(valid_edge_index[1,:])),\n",
    "          )\n",
    "      )  \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b401aa-63c5-4d54-9102-4e8c38edbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampling_sizes = {\n",
    "    \"bought_together\": 8,\n",
    "}\n",
    "\n",
    "def create_sampling_model(full_graph_tensor: tfgnn.GraphTensor, sizes: Mapping[str, int]) -> tf.keras.Model:\n",
    "\n",
    "    def edge_sampler(sampling_op: tfgnn.sampler.SamplingOp):    \n",
    "        edge_set_name = sampling_op.edge_set_name\n",
    "        sample_size = sizes[edge_set_name]\n",
    "        return sampler.InMemUniformEdgesSampler.from_graph_tensor(\n",
    "            full_graph_tensor,\n",
    "            edge_set_name, sample_size=sample_size\n",
    "        )\n",
    "    def get_features(node_set_name: tfgnn.NodeSetName):\n",
    "        return sampler.InMemIndexToFeaturesAccessor.from_graph_tensor(\n",
    "            full_graph_tensor,\n",
    "            node_set_name\n",
    "        )\n",
    "\n",
    "    sampling_spec_builder = tfgnn.sampler.SamplingSpecBuilder(graph_schema)\n",
    "    seed = sampling_spec_builder.seed(\"product\")\n",
    "    products_bought_together = seed.sample(sizes[\"bought_together\"], \"bought_together\", op_name=\"bt\")\n",
    "    sampling_spec = sampling_spec_builder.build()\n",
    "    model = sampler.create_sampling_model_from_spec(graph_schema, sampling_spec, edge_sampler, get_features, seed_node_dtype=tf.int64)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e329986-c235-4e22-b630-5e2299aaf963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def node_sets_fn(node_set, *, node_set_name):\n",
    "#     features = node_set.get_features_dict()\n",
    "#     ids = features.pop('id')\n",
    "#     num_bins = 50000\n",
    "#     features['hashed_id'] = tf.cast(tf.keras.layers.Hashing(num_bins=num_bins)(ids), tf.int32)\n",
    "#     return features\n",
    "# graph = tfgnn.keras.layers.MapFeatures(node_sets_fn=node_sets_fn)(graph_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2befa5b-ddb2-4906-a046-1af4a24abe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubgraphDatasetProvider(runner.DatasetProvider):\n",
    "    \"Dataset provider\"\n",
    "\n",
    "    def __init__(self,\n",
    "                full_graph_tensor: tfgnn.GraphTensor,\n",
    "                sizes: Mapping[str, int],\n",
    "                dataset: tf.data.Dataset):\n",
    "        self._sampling_model = create_sampling_model(full_graph_tensor, sizes)\n",
    "        self.input_graph_spec = self._sampling_model.output.spec\n",
    "        self._seed_dataset = dataset\n",
    "        \n",
    "    def get_dataset(self, context: tf.distribute.InputContext) -> tf.data.Dataset:\n",
    "        \"\"\"Creates TF dataset\"\"\"\n",
    "        ds = self._seed_dataset.shard(num_shards=context.num_input_pipelines, index = context.input_pipeline_id)\n",
    "        ds = ds.shuffle(NUM_TRAINING_SAMPLES).repeat()\n",
    "        ds = ds.batch(128)\n",
    "        ds = ds.map(\n",
    "            functools.partial(self.sample),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        return ds.unbatch().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def sample(self, seeds: tf.Tensor) -> tfgnn.GraphTensor:\n",
    "        # seeds = tf.cast(seeds, tf.int32)\n",
    "        batch_size = tf.size(seeds)\n",
    "        # print(f\"batch_size={batch_size}\")\n",
    "        seeds_ragged = tf.RaggedTensor.from_row_lengths(seeds, tf.ones([batch_size], dtype=tf.int64))\n",
    "        return self._sampling_model(seeds_ragged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b535af-439a-4a3e-ab4b-d961598f8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'tensorflow_gnn.graph.graph_tensor._ImmutableMapping'>\n",
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class 'tensorflow_gnn.graph.graph_tensor._ImmutableMapping'>\n"
     ]
    }
   ],
   "source": [
    "train_ds_provider = SubgraphDatasetProvider(graph_tensor, train_sampling_sizes, tf.data.Dataset.from_tensor_slices(train_idx))\n",
    "valid_ds_provider = SubgraphDatasetProvider(graph_tensor, train_sampling_sizes, tf.data.Dataset.from_tensor_slices(valid_idx))\n",
    "\n",
    "example_input_graph_spec = train_ds_provider.input_graph_spec._unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1db5e7ea-3f99-4dc6-bf6d-884b38be4b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MirroredStrategy for GPUs\n",
      "GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-b2dd31a4-1d4c-6d81-647d-e36ea0e64af9)\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Found 1 replicas in sync\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices(\"TPU\"):\n",
    "  print(f\"Using TPUStrategy\")\n",
    "  min_nodes_per_component = {\"paper\": 1}\n",
    "  strategy = runner.TPUStrategy(\"local\")\n",
    "  train_padding = runner.FitOrSkipPadding(example_input_graph_spec, train_ds_provider, min_nodes_per_component)\n",
    "  valid_padding = runner.TightPadding(example_input_graph_spec, valid_ds_provider, min_nodes_per_component)\n",
    "elif tf.config.list_physical_devices(\"GPU\"):\n",
    "  print(f\"Using MirroredStrategy for GPUs\")\n",
    "  gpu_list = !nvidia-smi -L\n",
    "  print(\"\\n\".join(gpu_list))\n",
    "  strategy = tf.distribute.MirroredStrategy()\n",
    "  train_padding = None\n",
    "  valid_padding = None\n",
    "else:\n",
    "  print(f\"Using default strategy\")\n",
    "  strategy = tf.distribute.get_strategy()\n",
    "  train_padding = None\n",
    "  valid_padding = None\n",
    "print(f\"Found {strategy.num_replicas_in_sync} replicas in sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce078a4-881d-4e45-b397-5ac24ab80865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_node_features(node_set: tfgnn.NodeSet, node_set_name: str):\n",
    "    if node_set_name == \"product\":\n",
    "        return {\"feature\": node_set[\"feature\"], \"label\": node_set[\"label\"]}\n",
    "    raise KeyError(f\"Unexpected node_set_name='{node_set_name}'\")\n",
    "\n",
    "def drop_all_features(_, **unused_kwargs):\n",
    "    return {}\n",
    "\n",
    "process_features = tfgnn.keras.layers.MapFeatures(\n",
    "    context_fn=drop_all_features,\n",
    "    node_sets_fn=process_node_features,\n",
    "    edge_sets_fn=drop_all_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2747cc15-3c37-4cb3-87e0-4a7a0108b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_readout = tfgnn.keras.layers.AddReadoutFromFirstNode(\"seed\", node_set_name=\"product\")\n",
    "move_label_to_readout = tfgnn.keras.layers.StructuredReadoutIntoFeature(\n",
    "    \"seed\", feature_name=\"label\", new_feature_name=\"category\", remove_input_feature=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec2b493-c949-44ec-b899-d103c06d5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_processors = [\n",
    "    process_features,\n",
    "    add_readout,\n",
    "    move_label_to_readout,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ab67318-bb0b-4a7e-b4b8-f9a5fd83721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_state_dim = 128\n",
    "\n",
    "def set_initial_node_states(node_set: tfgnn.NodeSet, node_set_name: str):\n",
    "    if node_set_name == \"product\":\n",
    "        return tf.keras.layers.Dense(node_state_dim, \"relu\")(node_set[\"feature\"])\n",
    "    raise KeyError(f\"Unexpected node_set_name='{node_set_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15b8eb9e-b87d-4ea6-8438-9e70f95ae0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graph_updates = 1\n",
    "message_dim = 128\n",
    "state_dropout_rate = 0.2\n",
    "l2_regularization= 1e-5\n",
    "\n",
    "def model_fn(graph_tensor_spec: tfgnn.GraphTensorSpec):\n",
    "    graph = inputs = tf.keras.layers.Input(type_spec=graph_tensor_spec)\n",
    "    graph = tfgnn.keras.layers.MapFeatures(\n",
    "        node_sets_fn = set_initial_node_states)(graph)\n",
    "    for i in range(num_graph_updates):\n",
    "        graph = mt_albis.MtAlbisGraphUpdate(\n",
    "            units = node_state_dim,\n",
    "            message_dim = message_dim,\n",
    "            receiver_tag = tfgnn.SOURCE,\n",
    "            node_set_names = None if i < num_graph_updates - 1 else [\"product\"],\n",
    "            simple_conv_reduce_type=\"mean|sum\",\n",
    "            state_dropout_rate=state_dropout_rate,\n",
    "            l2_regularization=l2_regularization,\n",
    "            normalization_type=\"layer\",\n",
    "            next_state_type=\"residual\",\n",
    "        )(graph)\n",
    "    return tf.keras.Model(inputs, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "376c89bd-9f35-4415-bfe1-ec70e5abd04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(label.flatten()).shape gives 47\n",
    "task = runner.NodeMulticlassClassification(\n",
    "    num_classes=47,\n",
    "    label_feature_name=\"category\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "405a5c0a-a498-488f-a28f-dfd784ba030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = 128\n",
    "epochs = 30\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "if tf.config.list_physical_devices(\"TPU\"):\n",
    "    epoch_divisor = 1\n",
    "else:\n",
    "    epoch_divisor = 1\n",
    "\n",
    "steps_per_epoch = NUM_TRAINING_SAMPLES // global_batch_size // epoch_divisor\n",
    "validation_steps = NUM_VALIDATION_SAMPLES // global_batch_size // epoch_divisor\n",
    "\n",
    "learning_rate = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate, steps_per_epoch*epochs)\n",
    "optimizer_fn = functools.partial(tf.keras.optimizers.Adam, learning_rate=learning_rate)\n",
    "\n",
    "trainer = runner.KerasTrainer(\n",
    "    strategy=strategy,\n",
    "    model_dir=\"/tmp/gnn_model\",\n",
    "    callbacks=[TensorBoard(log_dir=\"logs\")],\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    restore_best_weights=False,\n",
    "    checkpoint_every_n_steps=\"never\",\n",
    "    summarize_every_n_steps=\"never\",\n",
    "    backup_and_restore=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dd6d340-f68a-4c1a-bf6f-43352af624e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 14:16:24.179644: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2025-11-23 14:16:24.213582: I external/local_xla/xla/service/service.cc:163] XLA service 0x79b01898dd20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-23 14:16:24.213592: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-11-23 14:16:24.222185: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1763907384.292899    9319 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536/1536 [==============================] - 11s 6ms/step - loss: 0.7417 - sparse_categorical_accuracy: 0.7952 - sparse_categorical_crossentropy: 0.7374 - val_loss: 0.5436 - val_sparse_categorical_accuracy: 0.8523 - val_sparse_categorical_crossentropy: 0.5387\n",
      "Epoch 2/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.5018 - sparse_categorical_accuracy: 0.8601 - sparse_categorical_crossentropy: 0.4963 - val_loss: 0.4907 - val_sparse_categorical_accuracy: 0.8666 - val_sparse_categorical_crossentropy: 0.4846\n",
      "Epoch 3/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.4541 - sparse_categorical_accuracy: 0.8729 - sparse_categorical_crossentropy: 0.4474 - val_loss: 0.4706 - val_sparse_categorical_accuracy: 0.8718 - val_sparse_categorical_crossentropy: 0.4632\n",
      "Epoch 4/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.4244 - sparse_categorical_accuracy: 0.8812 - sparse_categorical_crossentropy: 0.4164 - val_loss: 0.4599 - val_sparse_categorical_accuracy: 0.8752 - val_sparse_categorical_crossentropy: 0.4513\n",
      "Epoch 5/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.4069 - sparse_categorical_accuracy: 0.8862 - sparse_categorical_crossentropy: 0.3978 - val_loss: 0.4458 - val_sparse_categorical_accuracy: 0.8791 - val_sparse_categorical_crossentropy: 0.4361\n",
      "Epoch 6/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3927 - sparse_categorical_accuracy: 0.8914 - sparse_categorical_crossentropy: 0.3826 - val_loss: 0.4419 - val_sparse_categorical_accuracy: 0.8803 - val_sparse_categorical_crossentropy: 0.4313\n",
      "Epoch 7/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3832 - sparse_categorical_accuracy: 0.8927 - sparse_categorical_crossentropy: 0.3723 - val_loss: 0.4368 - val_sparse_categorical_accuracy: 0.8848 - val_sparse_categorical_crossentropy: 0.4254\n",
      "Epoch 8/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3719 - sparse_categorical_accuracy: 0.8954 - sparse_categorical_crossentropy: 0.3602 - val_loss: 0.4398 - val_sparse_categorical_accuracy: 0.8836 - val_sparse_categorical_crossentropy: 0.4278\n",
      "Epoch 9/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.3651 - sparse_categorical_accuracy: 0.8986 - sparse_categorical_crossentropy: 0.3528 - val_loss: 0.4416 - val_sparse_categorical_accuracy: 0.8828 - val_sparse_categorical_crossentropy: 0.4290\n",
      "Epoch 10/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3559 - sparse_categorical_accuracy: 0.9006 - sparse_categorical_crossentropy: 0.3431 - val_loss: 0.4386 - val_sparse_categorical_accuracy: 0.8847 - val_sparse_categorical_crossentropy: 0.4256\n",
      "Epoch 11/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3501 - sparse_categorical_accuracy: 0.9024 - sparse_categorical_crossentropy: 0.3369 - val_loss: 0.4341 - val_sparse_categorical_accuracy: 0.8857 - val_sparse_categorical_crossentropy: 0.4207\n",
      "Epoch 12/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3434 - sparse_categorical_accuracy: 0.9040 - sparse_categorical_crossentropy: 0.3299 - val_loss: 0.4260 - val_sparse_categorical_accuracy: 0.8877 - val_sparse_categorical_crossentropy: 0.4123\n",
      "Epoch 13/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3361 - sparse_categorical_accuracy: 0.9064 - sparse_categorical_crossentropy: 0.3223 - val_loss: 0.4326 - val_sparse_categorical_accuracy: 0.8869 - val_sparse_categorical_crossentropy: 0.4187\n",
      "Epoch 14/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.3298 - sparse_categorical_accuracy: 0.9081 - sparse_categorical_crossentropy: 0.3158 - val_loss: 0.4362 - val_sparse_categorical_accuracy: 0.8875 - val_sparse_categorical_crossentropy: 0.4221\n",
      "Epoch 15/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3239 - sparse_categorical_accuracy: 0.9097 - sparse_categorical_crossentropy: 0.3097 - val_loss: 0.4298 - val_sparse_categorical_accuracy: 0.8886 - val_sparse_categorical_crossentropy: 0.4156\n",
      "Epoch 16/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3196 - sparse_categorical_accuracy: 0.9111 - sparse_categorical_crossentropy: 0.3053 - val_loss: 0.4288 - val_sparse_categorical_accuracy: 0.8892 - val_sparse_categorical_crossentropy: 0.4145\n",
      "Epoch 17/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.3137 - sparse_categorical_accuracy: 0.9125 - sparse_categorical_crossentropy: 0.2995 - val_loss: 0.4280 - val_sparse_categorical_accuracy: 0.8891 - val_sparse_categorical_crossentropy: 0.4138\n",
      "Epoch 18/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3097 - sparse_categorical_accuracy: 0.9142 - sparse_categorical_crossentropy: 0.2955 - val_loss: 0.4268 - val_sparse_categorical_accuracy: 0.8909 - val_sparse_categorical_crossentropy: 0.4125\n",
      "Epoch 19/30\n",
      "1536/1536 [==============================] - 9s 6ms/step - loss: 0.3058 - sparse_categorical_accuracy: 0.9151 - sparse_categorical_crossentropy: 0.2915 - val_loss: 0.4285 - val_sparse_categorical_accuracy: 0.8905 - val_sparse_categorical_crossentropy: 0.4143\n",
      "Epoch 20/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.3000 - sparse_categorical_accuracy: 0.9164 - sparse_categorical_crossentropy: 0.2858 - val_loss: 0.4284 - val_sparse_categorical_accuracy: 0.8884 - val_sparse_categorical_crossentropy: 0.4142\n",
      "Epoch 21/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.2957 - sparse_categorical_accuracy: 0.9172 - sparse_categorical_crossentropy: 0.2815 - val_loss: 0.4303 - val_sparse_categorical_accuracy: 0.8909 - val_sparse_categorical_crossentropy: 0.4161\n",
      "Epoch 22/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.9187 - sparse_categorical_crossentropy: 0.2779 - val_loss: 0.4255 - val_sparse_categorical_accuracy: 0.8913 - val_sparse_categorical_crossentropy: 0.4113\n",
      "Epoch 23/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.2902 - sparse_categorical_accuracy: 0.9189 - sparse_categorical_crossentropy: 0.2761 - val_loss: 0.4259 - val_sparse_categorical_accuracy: 0.8921 - val_sparse_categorical_crossentropy: 0.4118\n",
      "Epoch 24/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.2873 - sparse_categorical_accuracy: 0.9201 - sparse_categorical_crossentropy: 0.2732 - val_loss: 0.4257 - val_sparse_categorical_accuracy: 0.8930 - val_sparse_categorical_crossentropy: 0.4116\n",
      "Epoch 25/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.2832 - sparse_categorical_accuracy: 0.9211 - sparse_categorical_crossentropy: 0.2692 - val_loss: 0.4260 - val_sparse_categorical_accuracy: 0.8920 - val_sparse_categorical_crossentropy: 0.4120\n",
      "Epoch 26/30\n",
      "1536/1536 [==============================] - 10s 7ms/step - loss: 0.2798 - sparse_categorical_accuracy: 0.9215 - sparse_categorical_crossentropy: 0.2658 - val_loss: 0.4268 - val_sparse_categorical_accuracy: 0.8918 - val_sparse_categorical_crossentropy: 0.4128\n",
      "Epoch 27/30\n",
      "1536/1536 [==============================] - 10s 7ms/step - loss: 0.2791 - sparse_categorical_accuracy: 0.9220 - sparse_categorical_crossentropy: 0.2651 - val_loss: 0.4243 - val_sparse_categorical_accuracy: 0.8922 - val_sparse_categorical_crossentropy: 0.4102\n",
      "Epoch 28/30\n",
      "1536/1536 [==============================] - 10s 7ms/step - loss: 0.2787 - sparse_categorical_accuracy: 0.9221 - sparse_categorical_crossentropy: 0.2647 - val_loss: 0.4319 - val_sparse_categorical_accuracy: 0.8921 - val_sparse_categorical_crossentropy: 0.4179\n",
      "Epoch 29/30\n",
      "1536/1536 [==============================] - 10s 7ms/step - loss: 0.2758 - sparse_categorical_accuracy: 0.9225 - sparse_categorical_crossentropy: 0.2618 - val_loss: 0.4295 - val_sparse_categorical_accuracy: 0.8928 - val_sparse_categorical_crossentropy: 0.4155\n",
      "Epoch 30/30\n",
      "1536/1536 [==============================] - 10s 6ms/step - loss: 0.2772 - sparse_categorical_accuracy: 0.9223 - sparse_categorical_crossentropy: 0.2631 - val_loss: 0.4259 - val_sparse_categorical_accuracy: 0.8941 - val_sparse_categorical_crossentropy: 0.4119\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: /tmp/gnn_model/export/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/gnn_model/export/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RunResult(preprocess_model=<tf_keras.src.engine.functional.Functional object at 0x79b0b43dc2f0>, base_model=<tf_keras.src.engine.sequential.Sequential object at 0x79b0401f9e50>, trained_model=<tf_keras.src.engine.functional.Functional object at 0x79b15810d010>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_exporter = runner.KerasModelExporter(output_names=\"product_category\")\n",
    "runner.run(\n",
    "    gtspec=example_input_graph_spec,\n",
    "    train_ds_provider=train_ds_provider,\n",
    "    train_padding=train_padding,\n",
    "    # valid_ds_provider=None,\n",
    "    valid_ds_provider=valid_ds_provider,\n",
    "    valid_padding=valid_padding,\n",
    "    global_batch_size=global_batch_size,\n",
    "    epochs=epochs,\n",
    "    feature_processors=feature_processors,\n",
    "    model_fn=model_fn,\n",
    "    task=task,\n",
    "    optimizer_fn=optimizer_fn,\n",
    "    trainer=trainer,\n",
    "    model_exporters=[model_exporter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d4e0c0e-682e-40ce-87da-7ac3bcae6fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class for input 0 is   0 with predicted probability 0.9607\n",
      "The predicted class for input 1 is  13 with predicted probability 0.9999\n",
      "The predicted class for input 2 is   3 with predicted probability 0.9308\n",
      "The predicted class for input 3 is   4 with predicted probability 0.9997\n",
      "The predicted class for input 4 is  13 with predicted probability 0.9926\n",
      "The predicted class for input 5 is   3 with predicted probability 0.5268\n",
      "The predicted class for input 6 is   7 with predicted probability 0.9997\n",
      "The predicted class for input 7 is   4 with predicted probability 1.0\n",
      "The predicted class for input 8 is  10 with predicted probability 0.9874\n",
      "The predicted class for input 9 is   8 with predicted probability 0.9993\n"
     ]
    }
   ],
   "source": [
    "saved_model = tf.saved_model.load(os.path.join(trainer.model_dir, \"export\"))\n",
    "signature_fn = saved_model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "\n",
    "def _clean_example_for_serving(graph_tensor):\n",
    "    graph_tensor = graph_tensor.remove_features(node_sets={\"product\": [\"label\"]})\n",
    "    serialized_example = tfgnn.write_example(graph_tensor)\n",
    "    return serialized_example.SerializeToString()\n",
    "\n",
    "num_examples = 10\n",
    "demo_ds = valid_ds_provider.get_dataset(tf.distribute.InputContext())\n",
    "dds = itertools.islice(demo_ds, num_examples)\n",
    "serialized_examples = [_clean_example_for_serving(gt) for gt in dds]\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(serialized_examples)\n",
    "input_dict = {\"examples\": next(iter(ds.batch(num_examples)))}\n",
    "output_dict = signature_fn(**input_dict)\n",
    "logits = output_dict[\"product_category\"]\n",
    "probabilities = tf.math.softmax(logits).numpy()\n",
    "classes = probabilities.argmax(axis = 1)\n",
    "for i, c in enumerate(classes):\n",
    "    print(f\"The predicted class for input {i} is {c:3} \"\n",
    "          f\"with predicted probability {probabilities[i, c]:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a60982d-ae5f-49b6-8711-e7f1c563d462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
